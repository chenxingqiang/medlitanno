#!/usr/bin/env python3
"""
æ‰¹é‡å¤„ç†ç›‘æ§å’Œç®¡ç†è„šæœ¬
Batch processing monitoring and management script
"""

import os
import json
import time
import argparse
from datetime import datetime
from collections import defaultdict

def monitor_progress(data_dir="datatrain", refresh_interval=30):
    """
    å®æ—¶ç›‘æ§æ‰¹é‡å¤„ç†è¿›åº¦
    
    Args:
        data_dir: æ•°æ®ç›®å½•
        refresh_interval: åˆ·æ–°é—´éš”(ç§’)
    """
    print("=== æ‰¹é‡æ ‡æ³¨è¿›åº¦ç›‘æ§ ===")
    print("æŒ‰ Ctrl+C é€€å‡ºç›‘æ§")
    print()
    
    try:
        while True:
            # ç»Ÿè®¡æ€»æ–‡ä»¶æ•°
            total_files = 0
            for root, dirs, files in os.walk(data_dir):
                for file in files:
                    if file.endswith('.xlsx'):
                        total_files += 1
            
            # ç»Ÿè®¡å„æ¨¡å‹çš„å¤„ç†è¿›åº¦
            model_stats = {
                "deepseek": {"processed": 0, "total_articles": 0, "total_relations": 0},
                "deepseek-reasoner": {"processed": 0, "total_articles": 0, "total_relations": 0},
                "qianwen": {"processed": 0, "total_articles": 0, "total_relations": 0}
            }
            
            recent_files = []
            
            # éå†æ‰€æœ‰annotationç›®å½•
            for root, dirs, files in os.walk(data_dir):
                if 'annotation' in dirs:
                    annotation_dir = os.path.join(root, 'annotation')
                    for file in os.listdir(annotation_dir):
                        for model in model_stats.keys():
                            if file.endswith(f'_annotated_{model}.json'):
                                model_stats[model]["processed"] += 1
                                file_path = os.path.join(annotation_dir, file)
                                recent_files.append((file_path, os.path.getmtime(file_path)))
                                
                                # è¯»å–æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯
                                try:
                                    with open(file_path, 'r', encoding='utf-8') as f:
                                        data = json.load(f)
                                    model_stats[model]["total_articles"] += len(data)
                                    for article in data:
                                        model_stats[model]["total_relations"] += len(article.get('relations', []))
                                except:
                                    pass
            
            # æ¸…å±å¹¶æ˜¾ç¤ºè¿›åº¦
            os.system('clear' if os.name == 'posix' else 'cls')
            
            print("=== æ‰¹é‡æ ‡æ³¨è¿›åº¦ç›‘æ§ ===")
            print(f"â° ç›‘æ§æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"ğŸ“ æ€»æ–‡ä»¶æ•°: {total_files}")
            print()
            
            # æ˜¾ç¤ºå„æ¨¡å‹è¿›åº¦
            for model, stats in model_stats.items():
                processed = stats["processed"]
                progress_percent = (processed / total_files * 100) if total_files > 0 else 0
                
                print(f"ğŸ¤– {model.upper()}:")
                print(f"  å·²å¤„ç†: {processed}/{total_files} ({progress_percent:.1f}%)")
                
                # è¿›åº¦æ¡
                bar_length = 40
                filled_length = int(bar_length * processed // total_files) if total_files > 0 else 0
                bar = 'â–ˆ' * filled_length + '-' * (bar_length - filled_length)
                print(f"  è¿›åº¦æ¡: |{bar}| {progress_percent:.1f}%")
                
                if stats["total_articles"] > 0:
                    print(f"  æ ‡æ³¨æ–‡ç« : {stats['total_articles']}")
                    print(f"  æå–å…³ç³»: {stats['total_relations']}")
                
                print()
            
            # æ˜¾ç¤ºæœ€è¿‘å¤„ç†çš„æ–‡ä»¶
            if recent_files:
                recent_files.sort(key=lambda x: x[1], reverse=True)
                print("ğŸ“ æœ€è¿‘å¤„ç†çš„æ–‡ä»¶:")
                for file_path, mtime in recent_files[:5]:
                    rel_path = os.path.relpath(file_path, data_dir)
                    time_str = datetime.fromtimestamp(mtime).strftime('%H:%M:%S')
                    print(f"  - {rel_path} ({time_str})")
                print()
            
            # æ£€æŸ¥å¤±è´¥æ–‡ä»¶
            failed_logs = []
            for model in model_stats.keys():
                log_file = f"failed_files_{model}.json"
                if os.path.exists(log_file):
                    try:
                        with open(log_file, 'r', encoding='utf-8') as f:
                            failed_count = len(json.load(f))
                        failed_logs.append((model, failed_count))
                    except:
                        pass
            
            if failed_logs:
                print("âŒ å¤±è´¥æ–‡ä»¶:")
                for model, count in failed_logs:
                    print(f"  {model}: {count} ä¸ªæ–‡ä»¶")
                print()
            
            print("ğŸ’¡ æç¤º:")
            print("  - æŒ‰ Ctrl+C é€€å‡ºç›‘æ§")
            print("  - ç»“æœä¿å­˜åœ¨å„ç›®å½•çš„ annotation/ å­ç›®å½•ä¸‹")
            print("  - æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼Œå¯éšæ—¶ä¸­æ–­å’Œé‡å¯")
            
            # ç­‰å¾…åˆ·æ–°
            time.sleep(refresh_interval)
            
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç›‘æ§å·²é€€å‡º")

def check_status(data_dir="datatrain"):
    """
    æ£€æŸ¥å¤„ç†çŠ¶æ€
    
    Args:
        data_dir: æ•°æ®ç›®å½•
    """
    print("ğŸ“Š æ£€æŸ¥å¤„ç†çŠ¶æ€...")
    print(f"æ•°æ®ç›®å½•: {data_dir}")
    print()
    
    # ç»Ÿè®¡æ–‡ä»¶æ•°é‡
    total_files = 0
    processed_files = {"deepseek": 0, "deepseek-reasoner": 0, "qianwen": 0}
    model_stats = {"deepseek": {}, "deepseek-reasoner": {}, "qianwen": {}}
    
    for root, dirs, files in os.walk(data_dir):
        for file in files:
            if file.endswith('.xlsx'):
                total_files += 1
                
                # æ£€æŸ¥annotationç›®å½•
                annotation_dir = os.path.join(root, 'annotation')
                if os.path.exists(annotation_dir):
                    base_name = os.path.splitext(file)[0]
                    for model in processed_files.keys():
                        result_file = os.path.join(annotation_dir, f"{base_name}_annotated_{model}.json")
                        stats_file = os.path.join(annotation_dir, f"{base_name}_stats_{model}.json")
                        
                        if os.path.exists(result_file):
                            processed_files[model] += 1
                            
                            # è¯»å–ç»Ÿè®¡ä¿¡æ¯
                            if os.path.exists(stats_file):
                                try:
                                    with open(stats_file, 'r', encoding='utf-8') as f:
                                        stats = json.load(f)
                                    
                                    if 'total_articles' not in model_stats[model]:
                                        model_stats[model] = {
                                            'total_articles': 0,
                                            'total_bacteria': 0,
                                            'total_diseases': 0,
                                            'total_relations': 0,
                                            'relation_types': defaultdict(int)
                                        }
                                    
                                    model_stats[model]['total_articles'] += stats.get('total_articles', 0)
                                    model_stats[model]['total_bacteria'] += stats.get('total_bacteria', 0)
                                    model_stats[model]['total_diseases'] += stats.get('total_diseases', 0)
                                    model_stats[model]['total_relations'] += stats.get('total_relations', 0)
                                    
                                    for rel_type, count in stats.get('relation_types', {}).items():
                                        model_stats[model]['relation_types'][rel_type] += count
                                        
                                except:
                                    pass
    
    print(f"ğŸ“ æ€»æ–‡ä»¶æ•°: {total_files}")
    print()
    
    for model, count in processed_files.items():
        percentage = (count / total_files * 100) if total_files > 0 else 0
        print(f"ğŸ¤– {model.upper()}:")
        print(f"  å¤„ç†è¿›åº¦: {count}/{total_files} ({percentage:.1f}%)")
        
        if model in model_stats and model_stats[model]:
            stats = model_stats[model]
            print(f"  æ ‡æ³¨æ–‡ç« : {stats['total_articles']}")
            print(f"  ç»†èŒå®ä½“: {stats['total_bacteria']}")
            print(f"  ç–¾ç—…å®ä½“: {stats['total_diseases']}")
            print(f"  å…³ç³»æ€»æ•°: {stats['total_relations']}")
            
            if stats['relation_types']:
                print(f"  å…³ç³»ç±»å‹:")
                for rel_type, rel_count in sorted(stats['relation_types'].items()):
                    print(f"    - {rel_type}: {rel_count}")
        
        print()
    
    # æ£€æŸ¥å¤±è´¥æ–‡ä»¶æ—¥å¿—
    failed_logs = []
    for model in processed_files.keys():
        log_file = f"failed_files_{model}.json"
        if os.path.exists(log_file):
            try:
                with open(log_file, 'r', encoding='utf-8') as f:
                    failed_items = json.load(f)
                failed_logs.append((log_file, len(failed_items)))
                print(f"âŒ {model} å¤±è´¥æ–‡ä»¶: {len(failed_items)} (æ—¥å¿—: {log_file})")
            except:
                pass
    
    if failed_logs:
        print("\nğŸ’¡ é‡æ–°å¤„ç†å¤±è´¥æ–‡ä»¶çš„å‘½ä»¤:")
        for log_file, count in failed_logs:
            print(f"  python3 batch_monitor.py --retry-failed {log_file}")
    
    print(f"\nğŸ“ˆ æ€»ä½“è¿›åº¦:")
    total_processed = sum(processed_files.values())
    max_possible = total_files * len(processed_files)
    overall_percentage = (total_processed / max_possible * 100) if max_possible > 0 else 0
    print(f"  å·²å¤„ç†: {total_processed}/{max_possible} ({overall_percentage:.1f}%)")

def restart_processing(model="deepseek-reasoner", data_dir="datatrain"):
    """
    é‡æ–°å¯åŠ¨æ‰¹é‡å¤„ç†
    
    Args:
        model: æ¨¡å‹ç±»å‹
        data_dir: æ•°æ®ç›®å½•
    """
    print(f"ğŸš€ é‡æ–°å¯åŠ¨ {model} æ¨¡å‹çš„æ‰¹é‡å¤„ç†...")
    print(f"ğŸ“ æ•°æ®ç›®å½•: {data_dir}")
    print("ğŸ’¡ æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼Œå·²å¤„ç†çš„æ–‡ä»¶å°†è¢«è·³è¿‡")
    print()
    
    # é…ç½®APIå¯†é’¥
    api_keys = {
        "deepseek": "sk-d02fca54e07f4bdfb1778aeb62ae7671",
        "deepseek-reasoner": "sk-d02fca54e07f4bdfb1778aeb62ae7671",
        "qianwen": "sk-296434b603504719b9f5aca8286f5166"
    }
    
    model_names = {
        "deepseek": "deepseek-chat",
        "deepseek-reasoner": "deepseek-reasoner",
        "qianwen": "qwen-plus"
    }
    
    if model not in api_keys:
        print(f"âŒ ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model}")
        return
    
    try:
        from auto_annotation_system import batch_process_directory
        
        batch_process_directory(
            data_dir=data_dir,
            api_key=api_keys[model],
            model=model_names[model],
            model_type=model,
            max_retries=5,
            retry_delay=10
        )
        
    except KeyboardInterrupt:
        print("\nâš ï¸ å¤„ç†è¢«ä¸­æ–­")
        print("ğŸ’¡ å¯ä»¥ç¨åé‡æ–°è¿è¡Œç»§ç»­å¤„ç†")
    except Exception as e:
        print(f"âŒ å¤„ç†å‡ºç°é”™è¯¯: {e}")

def main():
    parser = argparse.ArgumentParser(description="æ‰¹é‡å¤„ç†ç›‘æ§å’Œç®¡ç†è„šæœ¬")
    parser.add_argument("--monitor", action="store_true", help="å®æ—¶ç›‘æ§å¤„ç†è¿›åº¦")
    parser.add_argument("--status", action="store_true", help="æ£€æŸ¥å¤„ç†çŠ¶æ€")
    parser.add_argument("--restart", type=str, choices=["deepseek", "deepseek-reasoner", "qianwen"],
                       help="é‡æ–°å¯åŠ¨æŒ‡å®šæ¨¡å‹çš„æ‰¹é‡å¤„ç†")
    parser.add_argument("--data-dir", type=str, default="datatrain", help="æ•°æ®ç›®å½• (é»˜è®¤: datatrain)")
    parser.add_argument("--refresh", type=int, default=30, help="ç›‘æ§åˆ·æ–°é—´éš”(ç§’) (é»˜è®¤: 30)")
    
    args = parser.parse_args()
    
    if args.monitor:
        monitor_progress(args.data_dir, args.refresh)
    elif args.status:
        check_status(args.data_dir)
    elif args.restart:
        restart_processing(args.restart, args.data_dir)
    else:
        print("æ‰¹é‡å¤„ç†ç›‘æ§å’Œç®¡ç†è„šæœ¬")
        print()
        print("å¯ç”¨æ“ä½œ:")
        print("  --monitor: å®æ—¶ç›‘æ§å¤„ç†è¿›åº¦")
        print("  --status: æ£€æŸ¥å¤„ç†çŠ¶æ€")
        print("  --restart <æ¨¡å‹>: é‡æ–°å¯åŠ¨æ‰¹é‡å¤„ç†")
        print()
        print("ç¤ºä¾‹:")
        print("  python3 batch_monitor.py --monitor")
        print("  python3 batch_monitor.py --status")
        print("  python3 batch_monitor.py --restart deepseek-reasoner")

if __name__ == "__main__":
    main() 